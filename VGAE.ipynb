{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b424744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from torch_geometric.nn import VGAE, GraphConv\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98e34eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab303/anaconda3/lib/python3.8/site-packages/torch_geometric/deprecation.py:13: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "# Normalization of adjacent matrix  T.NormalizeFeatures\n",
    "data = Planetoid(root='', name='Cora', transform=T.NormalizeFeatures())[0]\n",
    "data.train_mask = data.val_mask = data.test_mask = None\n",
    "data = train_test_split_edges(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d76e24c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], y=[2708], val_pos_edge_index=[2, 263], test_pos_edge_index=[2, 527], train_pos_edge_index=[2, 8976], train_neg_adj_mask=[2708, 2708], val_neg_edge_index=[2, 263], test_neg_edge_index=[2, 527])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pos and neg represent whether they exist in original graph.\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa6bd18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1433, device(type='cuda', index=0))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 200\n",
    "lr = 0.01\n",
    "hidden1 = 32\n",
    "hidden2 = 16\n",
    "# In fact ,there isn't dropout in paper, but when I reproduced without PyG, I added it, so it's reserved.\n",
    "\n",
    "input_channels = data.num_node_features\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "input_channels, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40b3cf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden1, hidden2, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        # first layer of GCN, share parameters for the second layer\n",
    "        self.conv1 = GraphConv(input_dim, hidden1)\n",
    "        # parameter of Gaussian distribution：mean mu\n",
    "        self.conv_mu = GraphConv(hidden1, hidden2)\n",
    "        # parameter of Gaussian distribution：log of standard deviation\n",
    "        self.conv_logstd = GraphConv(hidden1, hidden2)\n",
    "        self.dropout = dropout\n",
    "        self.edge_index = None\n",
    "\n",
    "    def conv_layer(self, x, conv):\n",
    "        # this function is redundant, you can merge it to 'forward' directly.\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        hidden = conv(x, self.edge_index)\n",
    "        hidden = F.relu(hidden, True)\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        self.edge_index = edge_index\n",
    "        hidden1 = self.conv_layer(x, self.conv1)\n",
    "        return self.conv_mu(hidden1, edge_index), self.conv_logstd(hidden1, edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "089d433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_channels, hidden1, hidden2, dropout)\n",
    "# decoder is InnerProduct by default, so I only add encoder\n",
    "model = VGAE(encoder).to(device)\n",
    "optimizer = Adam(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e78d26dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.x.to(device)\n",
    "train_pos_edge_index = data.train_pos_edge_index.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b03c0025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(x, train_pos_edge_index)\n",
    "    loss = model.recon_loss(z, train_pos_edge_index) + model.kl_loss() / data.num_nodes\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test(pos_edge_index, neg_edge_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # because this is a transductive task，it'll generate a complete graph using train_pos_edge.\n",
    "        z = model.encode(x, train_pos_edge_index)\n",
    "        # pos_edge represent existent edges，neg_edge represent inexistent edges\n",
    "        return model.test(z, pos_edge_index, neg_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2449b726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use tensorbaord to record the process of training\n",
    "writer = SummaryWriter('runs/VGAE_experiment_'+'200 epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "369019cf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, AUC: 0.5933, AP: 0.6404\n",
      "Epoch: 002, AUC: 0.5867, AP: 0.6374\n",
      "Epoch: 003, AUC: 0.5906, AP: 0.6390\n",
      "Epoch: 004, AUC: 0.6018, AP: 0.6453\n",
      "Epoch: 005, AUC: 0.6138, AP: 0.6564\n",
      "Epoch: 006, AUC: 0.6546, AP: 0.6947\n",
      "Epoch: 007, AUC: 0.7001, AP: 0.7233\n",
      "Epoch: 008, AUC: 0.7018, AP: 0.7170\n",
      "Epoch: 009, AUC: 0.7008, AP: 0.7191\n",
      "Epoch: 010, AUC: 0.7038, AP: 0.7311\n",
      "Epoch: 011, AUC: 0.6971, AP: 0.7354\n",
      "Epoch: 012, AUC: 0.6965, AP: 0.7360\n",
      "Epoch: 013, AUC: 0.7089, AP: 0.7416\n",
      "Epoch: 014, AUC: 0.7221, AP: 0.7480\n",
      "Epoch: 015, AUC: 0.7179, AP: 0.7466\n",
      "Epoch: 016, AUC: 0.7179, AP: 0.7467\n",
      "Epoch: 017, AUC: 0.7182, AP: 0.7478\n",
      "Epoch: 018, AUC: 0.7225, AP: 0.7528\n",
      "Epoch: 019, AUC: 0.7266, AP: 0.7571\n",
      "Epoch: 020, AUC: 0.7304, AP: 0.7610\n",
      "Epoch: 021, AUC: 0.7330, AP: 0.7645\n",
      "Epoch: 022, AUC: 0.7378, AP: 0.7686\n",
      "Epoch: 023, AUC: 0.7411, AP: 0.7704\n",
      "Epoch: 024, AUC: 0.7447, AP: 0.7721\n",
      "Epoch: 025, AUC: 0.7495, AP: 0.7758\n",
      "Epoch: 026, AUC: 0.7532, AP: 0.7797\n",
      "Epoch: 027, AUC: 0.7549, AP: 0.7818\n",
      "Epoch: 028, AUC: 0.7578, AP: 0.7847\n",
      "Epoch: 029, AUC: 0.7607, AP: 0.7865\n",
      "Epoch: 030, AUC: 0.7643, AP: 0.7885\n",
      "Epoch: 031, AUC: 0.7656, AP: 0.7912\n",
      "Epoch: 032, AUC: 0.7653, AP: 0.7932\n",
      "Epoch: 033, AUC: 0.7681, AP: 0.7967\n",
      "Epoch: 034, AUC: 0.7729, AP: 0.8022\n",
      "Epoch: 035, AUC: 0.7755, AP: 0.8051\n",
      "Epoch: 036, AUC: 0.7784, AP: 0.8077\n",
      "Epoch: 037, AUC: 0.7811, AP: 0.8098\n",
      "Epoch: 038, AUC: 0.7820, AP: 0.8100\n",
      "Epoch: 039, AUC: 0.7868, AP: 0.8136\n",
      "Epoch: 040, AUC: 0.7937, AP: 0.8191\n",
      "Epoch: 041, AUC: 0.8000, AP: 0.8237\n",
      "Epoch: 042, AUC: 0.8043, AP: 0.8270\n",
      "Epoch: 043, AUC: 0.8081, AP: 0.8292\n",
      "Epoch: 044, AUC: 0.8109, AP: 0.8313\n",
      "Epoch: 045, AUC: 0.8136, AP: 0.8333\n",
      "Epoch: 046, AUC: 0.8152, AP: 0.8344\n",
      "Epoch: 047, AUC: 0.8193, AP: 0.8364\n",
      "Epoch: 048, AUC: 0.8241, AP: 0.8390\n",
      "Epoch: 049, AUC: 0.8279, AP: 0.8413\n",
      "Epoch: 050, AUC: 0.8308, AP: 0.8436\n",
      "Epoch: 051, AUC: 0.8334, AP: 0.8455\n",
      "Epoch: 052, AUC: 0.8379, AP: 0.8485\n",
      "Epoch: 053, AUC: 0.8419, AP: 0.8510\n",
      "Epoch: 054, AUC: 0.8438, AP: 0.8525\n",
      "Epoch: 055, AUC: 0.8452, AP: 0.8537\n",
      "Epoch: 056, AUC: 0.8466, AP: 0.8545\n",
      "Epoch: 057, AUC: 0.8490, AP: 0.8562\n",
      "Epoch: 058, AUC: 0.8525, AP: 0.8587\n",
      "Epoch: 059, AUC: 0.8550, AP: 0.8606\n",
      "Epoch: 060, AUC: 0.8567, AP: 0.8623\n",
      "Epoch: 061, AUC: 0.8585, AP: 0.8640\n",
      "Epoch: 062, AUC: 0.8594, AP: 0.8647\n",
      "Epoch: 063, AUC: 0.8620, AP: 0.8670\n",
      "Epoch: 064, AUC: 0.8657, AP: 0.8700\n",
      "Epoch: 065, AUC: 0.8676, AP: 0.8714\n",
      "Epoch: 066, AUC: 0.8691, AP: 0.8720\n",
      "Epoch: 067, AUC: 0.8693, AP: 0.8719\n",
      "Epoch: 068, AUC: 0.8699, AP: 0.8721\n",
      "Epoch: 069, AUC: 0.8718, AP: 0.8736\n",
      "Epoch: 070, AUC: 0.8742, AP: 0.8750\n",
      "Epoch: 071, AUC: 0.8750, AP: 0.8754\n",
      "Epoch: 072, AUC: 0.8750, AP: 0.8753\n",
      "Epoch: 073, AUC: 0.8745, AP: 0.8748\n",
      "Epoch: 074, AUC: 0.8729, AP: 0.8734\n",
      "Epoch: 075, AUC: 0.8726, AP: 0.8730\n",
      "Epoch: 076, AUC: 0.8718, AP: 0.8726\n",
      "Epoch: 077, AUC: 0.8715, AP: 0.8730\n",
      "Epoch: 078, AUC: 0.8736, AP: 0.8747\n",
      "Epoch: 079, AUC: 0.8743, AP: 0.8750\n",
      "Epoch: 080, AUC: 0.8729, AP: 0.8737\n",
      "Epoch: 081, AUC: 0.8715, AP: 0.8723\n",
      "Epoch: 082, AUC: 0.8715, AP: 0.8727\n",
      "Epoch: 083, AUC: 0.8721, AP: 0.8733\n",
      "Epoch: 084, AUC: 0.8731, AP: 0.8744\n",
      "Epoch: 085, AUC: 0.8734, AP: 0.8742\n",
      "Epoch: 086, AUC: 0.8740, AP: 0.8747\n",
      "Epoch: 087, AUC: 0.8749, AP: 0.8755\n",
      "Epoch: 088, AUC: 0.8742, AP: 0.8747\n",
      "Epoch: 089, AUC: 0.8741, AP: 0.8747\n",
      "Epoch: 090, AUC: 0.8729, AP: 0.8737\n",
      "Epoch: 091, AUC: 0.8722, AP: 0.8732\n",
      "Epoch: 092, AUC: 0.8734, AP: 0.8744\n",
      "Epoch: 093, AUC: 0.8741, AP: 0.8749\n",
      "Epoch: 094, AUC: 0.8751, AP: 0.8756\n",
      "Epoch: 095, AUC: 0.8754, AP: 0.8761\n",
      "Epoch: 096, AUC: 0.8756, AP: 0.8766\n",
      "Epoch: 097, AUC: 0.8764, AP: 0.8775\n",
      "Epoch: 098, AUC: 0.8763, AP: 0.8776\n",
      "Epoch: 099, AUC: 0.8764, AP: 0.8779\n",
      "Epoch: 100, AUC: 0.8769, AP: 0.8787\n",
      "Epoch: 101, AUC: 0.8775, AP: 0.8796\n",
      "Epoch: 102, AUC: 0.8787, AP: 0.8810\n",
      "Epoch: 103, AUC: 0.8785, AP: 0.8806\n",
      "Epoch: 104, AUC: 0.8776, AP: 0.8793\n",
      "Epoch: 105, AUC: 0.8765, AP: 0.8784\n",
      "Epoch: 106, AUC: 0.8765, AP: 0.8793\n",
      "Epoch: 107, AUC: 0.8769, AP: 0.8800\n",
      "Epoch: 108, AUC: 0.8786, AP: 0.8813\n",
      "Epoch: 109, AUC: 0.8798, AP: 0.8815\n",
      "Epoch: 110, AUC: 0.8796, AP: 0.8804\n",
      "Epoch: 111, AUC: 0.8786, AP: 0.8800\n",
      "Epoch: 112, AUC: 0.8779, AP: 0.8796\n",
      "Epoch: 113, AUC: 0.8791, AP: 0.8813\n",
      "Epoch: 114, AUC: 0.8819, AP: 0.8834\n",
      "Epoch: 115, AUC: 0.8824, AP: 0.8832\n",
      "Epoch: 116, AUC: 0.8817, AP: 0.8827\n",
      "Epoch: 117, AUC: 0.8809, AP: 0.8825\n",
      "Epoch: 118, AUC: 0.8821, AP: 0.8839\n",
      "Epoch: 119, AUC: 0.8852, AP: 0.8863\n",
      "Epoch: 120, AUC: 0.8863, AP: 0.8872\n",
      "Epoch: 121, AUC: 0.8843, AP: 0.8864\n",
      "Epoch: 122, AUC: 0.8826, AP: 0.8855\n",
      "Epoch: 123, AUC: 0.8828, AP: 0.8860\n",
      "Epoch: 124, AUC: 0.8847, AP: 0.8873\n",
      "Epoch: 125, AUC: 0.8863, AP: 0.8879\n",
      "Epoch: 126, AUC: 0.8861, AP: 0.8881\n",
      "Epoch: 127, AUC: 0.8846, AP: 0.8877\n",
      "Epoch: 128, AUC: 0.8839, AP: 0.8874\n",
      "Epoch: 129, AUC: 0.8844, AP: 0.8878\n",
      "Epoch: 130, AUC: 0.8862, AP: 0.8892\n",
      "Epoch: 131, AUC: 0.8874, AP: 0.8905\n",
      "Epoch: 132, AUC: 0.8868, AP: 0.8905\n",
      "Epoch: 133, AUC: 0.8865, AP: 0.8902\n",
      "Epoch: 134, AUC: 0.8881, AP: 0.8918\n",
      "Epoch: 135, AUC: 0.8882, AP: 0.8920\n",
      "Epoch: 136, AUC: 0.8878, AP: 0.8919\n",
      "Epoch: 137, AUC: 0.8864, AP: 0.8912\n",
      "Epoch: 138, AUC: 0.8867, AP: 0.8918\n",
      "Epoch: 139, AUC: 0.8873, AP: 0.8926\n",
      "Epoch: 140, AUC: 0.8874, AP: 0.8926\n",
      "Epoch: 141, AUC: 0.8869, AP: 0.8925\n",
      "Epoch: 142, AUC: 0.8865, AP: 0.8928\n",
      "Epoch: 143, AUC: 0.8858, AP: 0.8923\n",
      "Epoch: 144, AUC: 0.8868, AP: 0.8936\n",
      "Epoch: 145, AUC: 0.8879, AP: 0.8944\n",
      "Epoch: 146, AUC: 0.8877, AP: 0.8944\n",
      "Epoch: 147, AUC: 0.8874, AP: 0.8945\n",
      "Epoch: 148, AUC: 0.8875, AP: 0.8945\n",
      "Epoch: 149, AUC: 0.8879, AP: 0.8950\n",
      "Epoch: 150, AUC: 0.8878, AP: 0.8949\n",
      "Epoch: 151, AUC: 0.8879, AP: 0.8950\n",
      "Epoch: 152, AUC: 0.8879, AP: 0.8952\n",
      "Epoch: 153, AUC: 0.8878, AP: 0.8951\n",
      "Epoch: 154, AUC: 0.8882, AP: 0.8955\n",
      "Epoch: 155, AUC: 0.8890, AP: 0.8960\n",
      "Epoch: 156, AUC: 0.8893, AP: 0.8964\n",
      "Epoch: 157, AUC: 0.8883, AP: 0.8962\n",
      "Epoch: 158, AUC: 0.8871, AP: 0.8952\n",
      "Epoch: 159, AUC: 0.8882, AP: 0.8955\n",
      "Epoch: 160, AUC: 0.8891, AP: 0.8964\n",
      "Epoch: 161, AUC: 0.8898, AP: 0.8976\n",
      "Epoch: 162, AUC: 0.8901, AP: 0.8978\n",
      "Epoch: 163, AUC: 0.8902, AP: 0.8980\n",
      "Epoch: 164, AUC: 0.8899, AP: 0.8976\n",
      "Epoch: 165, AUC: 0.8905, AP: 0.8982\n",
      "Epoch: 166, AUC: 0.8915, AP: 0.8991\n",
      "Epoch: 167, AUC: 0.8924, AP: 0.8998\n",
      "Epoch: 168, AUC: 0.8929, AP: 0.9001\n",
      "Epoch: 169, AUC: 0.8927, AP: 0.8997\n",
      "Epoch: 170, AUC: 0.8926, AP: 0.8988\n",
      "Epoch: 171, AUC: 0.8927, AP: 0.8988\n",
      "Epoch: 172, AUC: 0.8924, AP: 0.8988\n",
      "Epoch: 173, AUC: 0.8927, AP: 0.8993\n",
      "Epoch: 174, AUC: 0.8927, AP: 0.8991\n",
      "Epoch: 175, AUC: 0.8920, AP: 0.8981\n",
      "Epoch: 176, AUC: 0.8906, AP: 0.8965\n",
      "Epoch: 177, AUC: 0.8919, AP: 0.8980\n",
      "Epoch: 178, AUC: 0.8920, AP: 0.8983\n",
      "Epoch: 179, AUC: 0.8927, AP: 0.8989\n",
      "Epoch: 180, AUC: 0.8927, AP: 0.8991\n",
      "Epoch: 181, AUC: 0.8920, AP: 0.8984\n",
      "Epoch: 182, AUC: 0.8923, AP: 0.8985\n",
      "Epoch: 183, AUC: 0.8927, AP: 0.8989\n",
      "Epoch: 184, AUC: 0.8936, AP: 0.8993\n",
      "Epoch: 185, AUC: 0.8936, AP: 0.8995\n",
      "Epoch: 186, AUC: 0.8943, AP: 0.8998\n",
      "Epoch: 187, AUC: 0.8946, AP: 0.8999\n",
      "Epoch: 188, AUC: 0.8949, AP: 0.9001\n",
      "Epoch: 189, AUC: 0.8953, AP: 0.9003\n",
      "Epoch: 190, AUC: 0.8948, AP: 0.9000\n",
      "Epoch: 191, AUC: 0.8954, AP: 0.9003\n",
      "Epoch: 192, AUC: 0.8956, AP: 0.9011\n",
      "Epoch: 193, AUC: 0.8955, AP: 0.9013\n",
      "Epoch: 194, AUC: 0.8950, AP: 0.9010\n",
      "Epoch: 195, AUC: 0.8947, AP: 0.9000\n",
      "Epoch: 196, AUC: 0.8948, AP: 0.9000\n",
      "Epoch: 197, AUC: 0.8947, AP: 0.9006\n",
      "Epoch: 198, AUC: 0.8939, AP: 0.9008\n",
      "Epoch: 199, AUC: 0.8932, AP: 0.9002\n",
      "Epoch: 200, AUC: 0.8935, AP: 0.8996\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train()\n",
    "    auc, ap = test(data.test_pos_edge_index, data.test_neg_edge_index)\n",
    "    print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))\n",
    "    writer.add_scalar('auc train',auc,epoch)\n",
    "    writer.add_scalar('ap train',ap,epoch)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
